%% Copyright (C) 2008 Johan Oudinet <oudinet@lri.fr>
%%
%% Permission is granted to make and distribute verbatim copies of
%% this manual provided the copyright notice and this permission notice
%% are preserved on all copies.
%%
%% Permission is granted to process this file through TeX and print the
%% results, provided the printed document carries copying permission
%% notice identical to this one except for the removal of this paragraph
%% (this paragraph not being relevant to the printed manual).
%%
%% Permission is granted to copy and distribute modified versions of this
%% manual under the conditions for verbatim copying, provided that the
%% entire resulting derived work is distributed under the terms of a
%% permission notice identical to this one.
%%
%% Permission is granted to copy and distribute translations of this manual
%% into another language, under the above conditions for modified versions,
%% except that this permission notice may be stated in a translation
%% approved by the Free Software Foundation
%%
\chapter{Human Pose Estimation and COCO dataset}
\label{sec:intro}
In this chapter, several brief presentations will be given to explain the context of the human pose estimation and describe the COCO dataset.

\section{Human Pose Estimation}
\label{sec:isauriam}
Localizing body parts for human body is a fundamental yet challenging task in computer vision, and it serves as an important basis for high-level vision tasks, e.g., activity
recognition\cite{yang2010recognizing, wang2013approach}, human re-identification\cite{zheng2017pose}, and human-computer interaction.
In general，a human pose estimation model aims to predict the 2D coordinates of different human parts given a 2D human image.
Achieving accurate localization, however, is difficult due to the highly articulated human body limbs, occlusion, change of viewpoint, and foreshortening.

Classical approaches tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures \cite{fischler1973representation} or graphical models\cite{chen2014articulated}.
More specifically, the classical works\cite{andriluka2009pictorial, gkioxari2013articulated, sapp2013modec, johnson2011learning} formulate the problem of human keypoints estimation as a tree-structured or graphical model problem and predict keypoint locations based on hand-crafted features.
Recent works\cite{newell2016stacked, gkioxari2016chained, wei2016convolutional, insafutdinov2016deepercut} mostly rely on the development of convolutional neural network(CNN)\cite{lecun1998gradient, he2016deep}, which largely improve the performance of pose estimation.
And the rest of report also focuses on the solution based on CNN.

Nowadays there exists two main topics in human pose estimation: single person pose estimation and multi-person pose estimation. Obviously, multi-person is more challenging than single person pose estimaion but single person is the fundamentation for multi-person pose estimation, as shown in Figure.1.

\captionsetup[figure]{labelformat=empty}
\begin{figure}
  \centering
  \subfigure[Single person pose estimation]{
    \label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=5cm,height=4cm]{source/single.png}}
  \hspace{1in}
  \subfigure[Multi-person pose estimation]{
    \label{fig:subfig:b} %% label for second subfigure
    \includegraphics[width=5cm,height=4cm]{source/multi.png}}
  \caption{Figure 1: Human Pose Estimation. (a) is the example of single person pose estimation, only one person in a image. (b) is the example of multi-persion pose
  estimation. An image includes several peoples. We need to detect all the keypoints and group them into the right person ID.}
  \label{fig:1} %% label for entire figure
\end{figure}

\subsection{Single person Pose Estimation}
The works based on CNN usually adopts two methods: regress the coordinates of keypoints directly and regress the cofidence score map of keypoints. Toshev \textit{et al.} firstly introduce
CNN to solve pose estimation problem in the work of DeepPose\cite{toshev2014deeppose}, which proposes a cascade of CNN keypoint coordinate regressors to deal with pose estimation. Tompson \textit{et al.}\cite{tompson2014joint}
attempt to solve the problem by predicting heatmaps of keypoints using CNN and graphical models.
Using heatmap as the supervised label can provide more robust information, hence recently most of work focus on predicting heatmaps.
For example, latest works such as Wei \textit{et al.}\cite{wei2016convolutional} and Newell \textit{et al.}\cite{newell2016stacked} show great performance via generating the score map of keypoints using very
deep convolutional neural networks.

\subsection{Multi-person Pose estimation}
Multi-person pose estimation is gaining increasing popularity recently because of the high demand for the real-life applications.
However, multi-person pose estimation is challenging owing to occlusion, various gestures of individual persons and unpredictable interactions between different persons.
The approach of multi-person pose estimation is mainly divided into two categories: bottom-up approaches and top-down approaches.

\subsubsection{Bottom-Up Approaches}

Bottom-up approaches\cite{newell2017associative, insafutdinov2016deepercut, pishchulin2016deepcut} directly predict all keypoints at first and assemble them into full poses of all persons.
DeepCut\cite{pishchulin2016deepcut} interprets the problem of distinguishing different persons in an image as an Integer Linear Program (ILP) problem and partition
part detection candidates into person clusters.
Then the final pose estimation results are obtained when person clusters are combined with labeled body parts.
DeeperCut\cite{insafutdinov2016deepercut} improves DeepCut\cite{pishchulin2016deepcut} using deeper ResNet\cite{he2016deep} and employs image-conditioned pairwise terms to get better performance.
Zhe Cao \textit{et al.}\cite{cao2016realtime} map the relationship between keypoints into part affinity fields (PAFs) and assemble detected keypoints into different poses of people.
Newell \textit{et al.}\cite{newell2017associative} simultaneously produce score maps and pixel-wise embedding to group the candidate keypoints to different people to get final multi-person pose estimation.

\subsubsection{Top-down Approaches}
Top-down approaches\cite{huang2017coarse, papandreou2017towards, he2017mask} interpret the process of detecting keypoints as a twostage pipeline, that is, firstly locate and crop all persons from image,
and then solve the single person pose estimation problem in the cropped person patches.
Papandreou \textit{et al.}\cite{papandreou2017towards} predict both heatmaps and offsets of the points on the heatmaps to the ground truth location, and then uses the heatmaps with offsets to obtain the final predicted location of keypoints.
Mask-RCNN\cite{he2017mask} predicts human bounding boxes first and then crops the feature map of the corresponding human bounding box to predict human keypoints.
If top-down approach is utilized for multi-person pose estimation, a human detector as well as single person pose estimator is important in order to obtain a good performance.

\subsubsection{Human detection}
Human detection approaches are mainly guided by the RCNN family\cite{girshick2014rich, girshick2015fast, ren2015faster}, the upto-date detectors of which are\cite{lin2017feature, he2017mask}.
These detection approaches are composed of two-stage in general.
First generate boxes proposals based on default anchors, and then crop from the feature map and further refine the proposals to get the final boxes via R-CNN network.


\section{COCO dataset}

COCO\cite{lin2014microsoft} is a large-scale object detection, segmentation, and captioning dataset\footnote{More details about COCO dataset can be found in http://cocodataset.org/}.
For different tasks like object detection, keypoint detection, stuff segmentaion, etc, COCO dataset can be seperated as different sub-datasets.
Here we only introduce the human keypoint dataset.


\subsection{Dataset statistic}
The coco keypoint dataset was split up into 3 parts: train-set, validation-set, test-set.
And test-set is further divided into 2 parts: test-dev for evaluating the model in the usual time and test-challenge for evaluating the model during the competition.
It should be noted here that only labels of train-set and validation-set are available.
In order to extend the train-set, we extracted a small subset of the validation-set called mini-val for evaluating our model,
and the rest of validation-set and train-set merged into one larger train-set called train-val.

\captionsetup[table]{labelformat=empty}
\begin{table}[!hbp]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
            & train-set & validation-set & test-dev & test-challenge & train-val & mini-val \\
  \hline
  images numbers & 39935 & 19010 & 20000 & 20000 & 56599 & 2346\\
  \hline
  instances numbers & 105968 & 50197 & unknown & unknown & 149813 & 6352\\
  \hline
  keypoints numbers & 1161667  & 548831 & unknown & unknown & 1642283 & 68215\\
  \hline
  \end{tabular}
  \caption{Table 1: The statistics of datasets}
\end{table}

\subsection{Annotation format}

Each person(instance) annotations contains a series of fields. \textbf{1.} id: Indicates the global id of this instance. \textbf{2.} image\_id: Indicates which images this person belongs to. \textbf{3.} bbox: Indicates the location of this person in the image.
\textbf{4.} keypoints: A length 3*17 array, indicates the details of the keypoints. Each keypoint has a 0-indexed location x,y and a visibility flag v defined as v=0: not labeled (in which case x=y=0), v=1: labeled but not visible, and v=2: labeled and visible.
\textbf{5.} s: Indicates the square root of the object segment area.

There are 17 keypoints: 0: nose, 1: left eye, 2: right eye, 3: left ear, 4: right ear, 5: left shoulder, 6: right shoulder, 7: left elbow, 8: right elbow, 9: left wrist, 10: right wrist, 11: left hip, 12: right hip, 13: left knee, 14: right knee, 15: left ankle, 16: right ankle.

\begin{lstlisting}
annotation{
  "id" : int,
  "image_id" : int,
  "bbox" : [x, y, weight, height],
  // x, y is the coordinate of the top-left point of the bbox.
  "keypoints" : [x1, y1, v1, ...]
  "s" : float
}
\end{lstlisting}

\subsection{Metric}

The core idea behind evaluating keypoint detection is to mimic the evaluation metrics used for object detection, namely average precision (AP) and average recall (AR) and their variants.
At the heart of these metrics is a similarity measure between ground truth objects and predicted objects.
In the case of object detection, the IoU(Intersection over Union) serves as this similarity measure (for both boxes and segments).
Thesholding the IoU defines matches between the ground truth and predicted objects and allows computing precision-recall curves.
To adopt AP/AR for keypoints detection, we only need to define an analogous similarity measure.
Here an object keypoint similarity (OKS) which plays the same role as the IoU.

As mentioned before, for each object, ground truth keypoints have the form $[x_{1},y_{1},v_{1},...,x_{k},y_{k},v_{k}]$, where x,y are the keypoint locations and v is a visibility flag defined as v=0: not labeled, v=1: labeled but not visible, and v=2: labeled and visible.
Each ground truth object also has a scale s which is defined as the square root of the object segment area.

For each object, the keypoint detector must output keypoint locations and an object-level confidence.
Predicted keypoints for an object should have the same form as the ground truth: $[x_{1},y_{1},v_{1},...,x_{k},y_{k},v_{k}]$.
However, the detector's predicted $v_{i}$ are not currently used during evaluation, that is the keypoint detector is not required to predict per-keypoint visibilities or confidences.

The object keypoint similarity (OKS) is defined as:

\[ OKS =  \frac{\sum_{i}[exp(-d_{i}^{2} / 2s^{2}k_{i}^{2}) \delta (v_{i}>0)]}{\sum_{i} [\delta(v_{i}>0)]} ...................(1) \]

The $d_{i}$ are the Euclidean distances between each corresponding ground truth and detected keypoint and the $v_{i}$ are the visibility flags of the ground truth (the detector's predicted $v_{i}$ are not used).
To compute OKS, we pass the $d_{i}$ through an unnormalized Guassian with standard deviation $sk_{i}$, where s is the object scale and $k_{i}$ is a per-keypont constant that controls falloff.
For each keypoint this yields a keypoint similarity that ranges between 0 and 1.
These similarities are averaged over all labeled keypoints (keypoints for which $v_{i}>0$).
Predicted keypoints that are not labeled ($v_{i}=0$) do not affect the OKS.
Perfect predictions will have OKS=1 and predictions for which all keypoints are off by more than a few standard deviations sκi will have OKS~0.
The OKS is analogous to the IoU.
Given the OKS, we can compute AP and AR just as the IoU allows us to compute these metrics for box/segment detection.

The final metric for COCO keypoint dataset is shown as below:

\captionsetup[figure]{labelformat=empty}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=16cm,height=6cm]{source/metric.png}
  \caption{Figure 2: The final metrics for COCO keypoint task. }
\end{figure}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "rapportM2R"
%%% End:
